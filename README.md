**Name:** Purushottam Tiwari
**Roll No.:** 19074029
**Branch:** CSE(IDD)

### Description of the Lab Task:
The aim of this task was ***Morphological analysis of isolated words***.

**NOTE:** This task was done with colab so whole code is also available in following drive folder: [https://drive.google.com/drive/folders/1RFRdstgsl4_2K1FHo3bsf-XZrZuiAl4T?usp=sharing](https://drive.google.com/drive/folders/1RFRdstgsl4_2K1FHo3bsf-XZrZuiAl4T?usp=sharing)

**Note:** The extracted datasets are also available at:
[https://drive.google.com/drive/folders/1sBosbRXoSRa_US1uD-Kz7VAwUKV_mtSa?usp=sharing](https://drive.google.com/drive/folders/1sBosbRXoSRa_US1uD-Kz7VAwUKV_mtSa?usp=sharing).
Put these datasets in `src/` folder  in order to retrain the model.

## Morphological analysis:
Morphological analysis refers to the task of assigning a set of well-defined morphologicaltags (hereby, referred to as tags) and a lemma (root) to the tokens of a language by studying a range of syntactic traits such as inflection, derivation, combining forms, and cliticization. According to the classical approach in linguistics, words are formed of morphemes, which are the minimal (that is, non-‐decomposable) linguitic units  that carry meaning.
I have first dscribed the data processing part and then model.
#### Problem formulation:
This problem of morphological analysis can be visualized as a multi-task learning problem. Formally, our model learns a shared representation of features across the six morphological tags in addition to learning separate feature representation for lemma of a word by minimizing the joint categorical cross-entropyloss functions for all seven tasks.

### Model architecture and working:
It integrates two components: (a) **Tag Predictor**, and (b)**Lemma Predictor**. The input to the model comprises the current word’wrd’and a sequence of up to (2*CW) words, where CW stands for the size of the Context Window. Best results areobtained with CW = 4. The final output consists of the predicted set of tags - POS, G, N, P, C, andTAM, yielded by the tag predictor, as well as the Lemma (L), generated by the lemma predictor forward.

#### => Tag predictor
This model employs a CNN-RNN model for thesequential modeling problem of tag prediction. Simply put, each word is converted into a series of 4 and 5 ngrams (by applying convolution onthe character embeddings for each word). Max pooling and average pooling operations are carriedout in parallel upon the outputs of the convolution layers to create fix-sized vectors independentof the word length.

#### => Lemma predictor
The lemma predictor is essentially a seq2seq variant of the encoder-decoder model that shares the same embedding space as the tag predictor network to perform character-level transliterationof word. Unlike the tag predictor, the input to the lemma predictor model comprises only of thecurrent word, and not its context.

### Experiment:
* #### Dataset: 
    We use the dependency treebanks of Hindi(HDTB)and Urdu(UDTB) for our data. The Hindidatabase was downloaded from IIIT Hyderabad’s website which at the time of writing, hosts theversion 0.05 of the HDTB pre-release.
* #### Results: 
    Due to computational limitations I have not done whole evaluation but a similar work to which most of this experiment refers performs evaluation in following way:
    - For Hindi, the result is compared with two other functionalanalyzers hosted by: (A) IIT-Bombay (IIT-B)18and (B) IIIT-Hyderabad (IIIT-H)19. 
    - In absence of such a functional analyzer for Urdu, we set up the version 1.4 of the morph analyzer download available on the web page of Indian Language Technology Pro-liferation and Deployment Centre (TDIL).
* **Results for Hindi: best scores reported in bold across the experiments, where MT-DMA is the performance of our model**
![](https://firebasestorage.googleapis.com/v0/b/birthday-218c6.appspot.com/o/ai_lab_5%2Fai_hindi_results.png?alt=media&token=4c9c2c6f-845d-4dbd-bff7-018ac61dbe2e)
* **Results for urdu: best scores reported in bold across the experiments, where MT-DMA is the performance of our model**
![](https://firebasestorage.googleapis.com/v0/b/birthday-218c6.appspot.com/o/ai_lab_5%2Fai_urdu.png?alt=media&token=7641e113-4780-45e0-b6d8-70998f08075b)

**NOTE:** This description refers to [https://arxiv.org/pdf/1811.08619.pdf](https://arxiv.org/pdf/1811.08619.pdf) to a great extent, especially for the results(evaluation) section, because data(network) constraints I could not perform the ask of evaluation again but as the model is same(as the reference) so results is highly likely tto be almost same. Following were the precision scores while running my model on test data:
* For urdu:
    - Average precision score, micro-averaged over all classes: 0.455901189632699
* For hindi:
    - Average precision score, micro-averaged over all classes: 0.5843650113916106
